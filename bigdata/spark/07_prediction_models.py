#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ­¥éª¤7ï¼šé¢„æµ‹æ¨¡å‹åˆ†æ

åŠŸèƒ½ï¼š
1. é€¾æœŸé£é™©é¢„æµ‹ - ä½¿ç”¨éšæœºæ£®æ—é¢„æµ‹ç”¨æˆ·é€¾æœŸæ¦‚ç‡
2. å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹ - ä½¿ç”¨æ—¶é—´åºåˆ—é¢„æµ‹æœªæ¥å€Ÿé˜…é‡
3. å›¾ä¹¦çƒ­åº¦é¢„æµ‹ - é¢„æµ‹å›¾ä¹¦æœªæ¥çƒ­åº¦

è¾“å‡ºè¡¨ï¼š
- Hive: library_ads.ads_overdue_prediction, ads_lend_trend_prediction, ads_book_heat_prediction
- MySQL: overdue_risk_prediction, lend_trend_prediction, book_heat_prediction
"""

import os
import sys
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import col, lit, when, count, avg, sum as spark_sum, max as spark_max, min as spark_min
from pyspark.sql.functions import datediff, to_date, date_sub, date_add, month, year, dayofweek
from pyspark.sql.functions import lag, lead, round as spark_round, expr
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, LongType
from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler
from pyspark.ml.classification import RandomForestClassifier, GBTClassifier
from pyspark.ml.regression import RandomForestRegressor, GBTRegressor
from pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator
from pyspark.ml import Pipeline
import builtins


class PredictionModels:
    """é¢„æµ‹æ¨¡å‹åˆ†æç±»"""
    
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("PredictionModels") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .enableHiveSupport() \
            .getOrCreate()
        
        # MySQLè¿æ¥é…ç½®
        self.mysql_url = "jdbc:mysql://{}:{}/{}".format(
            os.getenv("MYSQL_HOST", "master"),
            os.getenv("MYSQL_PORT", "3306"),
            os.getenv("MYSQL_DATABASE", "library_analysis")
        )
        self.mysql_properties = {
            "user": os.getenv("MYSQL_USER", "root"),
            "password": os.getenv("MYSQL_PASSWORD", "123456"),
            "driver": "com.mysql.cj.jdbc.Driver"
        }
        
        print(f"MySQLè¿æ¥: {os.getenv('MYSQL_HOST', 'master')}:{os.getenv('MYSQL_PORT', '3306')}/{os.getenv('MYSQL_DATABASE', 'library_analysis')}")
        
        # æ•°æ®é›†
        self.lend_detail = None
        self.user_summary = None
        self.book_summary = None
        self.user_info = None
        self.book_info = None
        self.latest_date = None
        
        # æ¨¡å‹è¯„ä¼°ç»“æœï¼ˆç”¨äºæœ€åç»Ÿä¸€æ‰“å°ï¼‰
        self.evaluation_results = {
            "overdue_risk": {},
            "lend_trend": {},
            "book_heat": {}
        }
    
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print("\n" + "=" * 60)
        print("åŠ è½½æ•°æ®...")
        
        # åŠ è½½DWDå±‚æ•°æ®
        self.lend_detail = self.spark.table("library_dwd.dwd_lend_detail")
        self.user_info = self.spark.table("library_dwd.dwd_user_info")
        self.book_info = self.spark.table("library_dwd.dwd_book_info")
        
        # åŠ è½½DWSå±‚æ±‡æ€»æ•°æ®
        self.user_summary = self.spark.table("library_dws.dws_user_lend_summary")
        self.book_summary = self.spark.table("library_dws.dws_book_lend_summary")
        
        # è·å–æ•°æ®é›†æœ€æ–°æ—¥æœŸï¼ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        latest_date_result = self.lend_detail.agg(spark_max("lend_date")).collect()
        latest_date_value = latest_date_result[0][0] if latest_date_result else None
        if latest_date_value is None:
            self.latest_date = "2020-12-31"
        elif isinstance(latest_date_value, str):
            self.latest_date = latest_date_value
        else:
            # datetime.date ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            self.latest_date = latest_date_value.strftime("%Y-%m-%d")
        
        print(f"å€Ÿé˜…è®°å½•æ•°: {self.lend_detail.count():,}")
        print(f"ç”¨æˆ·æ±‡æ€»æ•°: {self.user_summary.count():,}")
        print(f"å›¾ä¹¦æ±‡æ€»æ•°: {self.book_summary.count():,}")
        print(f"æ•°æ®é›†æœ€æ–°æ—¥æœŸ: {self.latest_date}")
    
    def predict_overdue_risk(self):
        """
        é€¾æœŸé£é™©é¢„æµ‹ - ä½¿ç”¨éšæœºæ£®æ—é¢„æµ‹ç”¨æˆ·é€¾æœŸæ¦‚ç‡
        
        æ ¸å¿ƒæ€è·¯ï¼šåŸºäºç”¨æˆ·å†å²æœŸè¡Œä¸ºç‰¹å¾é¢„æµ‹å…¶è¿‘æœŸé€¾æœŸå€¾å‘
        
        æ—¶é—´åˆ’åˆ†ï¼š
        - å†å²æœŸï¼ˆç‰¹å¾ï¼‰ï¼š6ä¸ªæœˆå‰ä»¥å‰çš„å€Ÿé˜…è¡Œä¸º
        - è¿‘æœŸï¼ˆæ ‡ç­¾ï¼‰ï¼šæœ€è¿‘6ä¸ªæœˆæ˜¯å¦é€¾æœŸ
        
        æ³¨æ„ï¼šè¿™æ˜¯å›æµ‹ï¼ˆBacktestingï¼‰æ¨¡å¼
        - è®­ç»ƒé›† = é¢„æµ‹é›†ï¼ˆç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼‰
        - è¾“å‡ºçš„é¢„æµ‹ç»“æœæ˜¯å¯¹"å·²çŸ¥ç»“æœ"çš„é¢„æµ‹
        - å®é™…åº”ç”¨æ—¶ï¼Œåº”è¯¥ç”¨å…¨éƒ¨å†å²æ•°æ®è®­ç»ƒï¼Œå¯¹å½“å‰ç”¨æˆ·é¢„æµ‹æœªæ¥é£é™©
        
        æ¨¡å‹å­¦ä¹ çš„æ˜¯"ä»€ä¹ˆæ ·çš„å†å²è¡Œä¸ºæ¨¡å¼ä¼šå¯¼è‡´è¿‘æœŸé€¾æœŸ"
        """
        print("\n" + "=" * 60)
        print("[1/3] é€¾æœŸé£é™©é¢„æµ‹ï¼ˆéšæœºæ£®æ—ï¼‰...")
        
        # 1. æ—¶é—´åˆ’åˆ†ï¼šå‰æœŸç‰¹å¾ vs è¿‘æœŸæ ‡ç­¾
        split_date = (datetime.strptime(self.latest_date, "%Y-%m-%d") - timedelta(days=180)).strftime("%Y-%m-%d")
        print(f"  æ—¶é—´åˆ’åˆ†ç‚¹: {split_date}")
        
        # 2. è®¡ç®—å†å²ç‰¹å¾ï¼ˆ6ä¸ªæœˆå‰ä»¥å‰çš„å€Ÿé˜…è¡Œä¸ºï¼‰
        # æ˜¾å¼è½¬æ¢æ—¥æœŸç±»å‹ç¡®ä¿æ¯”è¾ƒæ­£ç¡®
        historical_lend = self.lend_detail.filter(col("lend_date") < to_date(lit(split_date)))
        
        historical_user_stats = historical_lend.groupBy("userid").agg(
            count("*").alias("historical_borrow_count"),
            F.countDistinct("lend_date").alias("historical_active_days"),
            avg("borrow_days").alias("historical_avg_borrow_days"),
            spark_sum(when(col("renew_times") > 0, 1).otherwise(0)).alias("historical_renew_count"),
            spark_sum(when(col("is_overdue") == 1, 1).otherwise(0)).alias("historical_overdue_count")
        )
        
        # è®¡ç®—å†å²é€¾æœŸç‡ï¼ˆç”¨äºç‰¹å¾ï¼‰
        historical_user_stats = historical_user_stats.withColumn(
            "historical_overdue_rate",
            when(col("historical_borrow_count") > 0,
                 col("historical_overdue_count") / col("historical_borrow_count"))
            .otherwise(0.0)
        )
        
        # 3. è®¡ç®—è¿‘æœŸæ ‡ç­¾ï¼ˆæœ€è¿‘6ä¸ªæœˆçš„é€¾æœŸæƒ…å†µï¼‰
        recent_lend = self.lend_detail.filter(col("lend_date") >= to_date(lit(split_date)))
        
        # åŒæ—¶è®¡ç®—è¿‘æœŸé€¾æœŸæ¬¡æ•°å’Œå€Ÿé˜…æ€»æ•°
        recent_stats = recent_lend.groupBy("userid").agg(
            spark_sum(when(col("is_overdue") == 1, 1).otherwise(0)).alias("recent_overdue_count"),
            count("*").alias("recent_borrow_count")
        )
        
        # 4. åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
        # ä½¿ç”¨inner joinåªä¿ç•™è¿‘æœŸæœ‰å€Ÿé˜…çš„ç”¨æˆ·ï¼ˆæœ‰å®é™…è¡Œä¸ºæ‰èƒ½è¯„ä¼°é£é™©ï¼‰
        user_features = historical_user_stats \
            .join(recent_stats, "userid", "inner") \
            .join(
                self.user_info.groupBy("userid").agg(
                    F.first("dept").alias("dept"),
                    F.first("redr_type_name").alias("user_type")
                ),
                "userid",
                "left"
            )
        
        # 5. è®¡ç®—è¡Œä¸ºç‰¹å¾
        user_features = user_features \
            .withColumn(
                "borrow_frequency",
                when(col("historical_active_days") > 0, 
                     col("historical_borrow_count") / col("historical_active_days")).otherwise(0.0)
            ) \
            .withColumn(
                "renew_ratio",
                when(col("historical_borrow_count") > 0, 
                     col("historical_renew_count") / col("historical_borrow_count")).otherwise(0.0)
            ) \
            .withColumn(
                "historical_avg_borrow_days",
                F.coalesce(col("historical_avg_borrow_days"), lit(0.0))
            )
        
        # 6. åˆ›å»ºæ ‡ç­¾ï¼šè¿‘æœŸé€¾æœŸç‡ï¼ˆè€Œéç®€å•çš„æ˜¯å¦é€¾æœŸï¼‰
        # è®¡ç®—è¿‘æœŸé€¾æœŸç‡ï¼ˆæ‰€æœ‰ç”¨æˆ·éƒ½æœ‰recent_borrow_count > 0ï¼Œå› ä¸ºç”¨äº†inner joinï¼‰
        user_features = user_features.withColumn(
            "recent_overdue_rate",
            col("recent_overdue_count") / col("recent_borrow_count")
        )
        
        # å®šä¹‰é«˜é£é™©ï¼šè¿‘æœŸé€¾æœŸç‡ > 20% æˆ– é€¾æœŸæ¬¡æ•° >= 2
        user_features = user_features.withColumn(
            "is_high_risk",
            when((col("recent_overdue_rate") > 0.2) | (col("recent_overdue_count") >= 2), 1.0)
            .otherwise(0.0)
        )
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®ï¼ˆå†å²æœŸè‡³å°‘æœ‰å€Ÿé˜…è®°å½•ï¼‰
        user_features = user_features.filter(col("historical_borrow_count") > 0)
        
        print(f"  è®­ç»ƒæ ·æœ¬æ•°: {user_features.count():,}")
        
        # ç»Ÿè®¡æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
        label_dist = user_features.groupBy("is_high_risk").count().collect()
        positive_count = 0
        negative_count = 0
        for row in label_dist:
            if row["is_high_risk"] == 1.0:
                positive_count = row['count']
                print(f"    æœ‰é€¾æœŸ: {row['count']:,}äºº")
            else:
                negative_count = row['count']
                print(f"    æ— é€¾æœŸ: {row['count']:,}äºº")
        
        # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†æ ·æœ¬ä¸å¹³è¡¡ï¼‰
        total_count = positive_count + negative_count
        if positive_count > 0 and negative_count > 0:
            weight_ratio = negative_count / positive_count
            print(f"    æ ·æœ¬ä¸å¹³è¡¡æ¯”ä¾‹: 1:{weight_ratio:.2f}")
            
            # ä¸ºå°‘æ•°ç±»ï¼ˆé€¾æœŸç”¨æˆ·ï¼‰æ·»åŠ æƒé‡
            user_features = user_features.withColumn(
                "sample_weight",
                when(col("is_high_risk") == 1.0, weight_ratio).otherwise(1.0)
            )
        else:
            weight_ratio = 1.0
            user_features = user_features.withColumn("sample_weight", lit(1.0))
        
        # 7. ç‰¹å¾å·¥ç¨‹ - åªä½¿ç”¨å†å²æœŸè¡Œä¸ºç‰¹å¾ï¼ˆä¸åŒ…å«å†å²é€¾æœŸç‡ï¼Œé¿å…è¿‡æ‹Ÿåˆï¼‰
        # æ³¨æ„ï¼šä¸ä½¿ç”¨historical_overdue_rateï¼Œå› ä¸ºå®ƒä¸æ ‡ç­¾é«˜åº¦ç›¸å…³ï¼Œä¼šå¯¼è‡´è¿‡æ‹Ÿåˆ
        # 
        # ç‰¹å¾è¯´æ˜ï¼š
        # - historical_borrow_count, historical_active_days: åŸå§‹ç‰¹å¾
        # - borrow_frequency: æ´¾ç”Ÿç‰¹å¾ = historical_borrow_count / historical_active_days
        # - renew_ratio: æ´¾ç”Ÿç‰¹å¾ = historical_renew_count / historical_borrow_count
        # 
        # è™½ç„¶å­˜åœ¨çº¿æ€§ä¾èµ–ï¼Œä½†éšæœºæ£®æ—å¯¹å¤šé‡å…±çº¿æ€§ä¸æ•æ„Ÿï¼Œå¯ä»¥ä¿ç•™
        feature_cols = ["historical_borrow_count", "historical_avg_borrow_days", "historical_renew_count", 
                        "historical_active_days", "borrow_frequency", "renew_ratio"]
        
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol="features_raw"
        )
        
        scaler = StandardScaler(
            inputCol="features_raw",
            outputCol="features",
            withStd=True,
            withMean=True
        )
        
        # 5. éšæœºæ£®æ—åˆ†ç±»å™¨ï¼ˆæ·»åŠ ç±»åˆ«æƒé‡å¤„ç†æ ·æœ¬ä¸å¹³è¡¡ï¼‰
        # ä½¿ç”¨weightColæ¥å¤„ç†ä¸å¹³è¡¡é—®é¢˜
        rf = RandomForestClassifier(
            featuresCol="features",
            labelCol="is_high_risk",
            predictionCol="prediction",
            probabilityCol="probability",
            weightCol="sample_weight",  # ä½¿ç”¨æ ·æœ¬æƒé‡
            numTrees=50,
            maxDepth=5,
            seed=42
        )
        
        # 6. æ„å»ºPipeline
        pipeline = Pipeline(stages=[assembler, scaler, rf])
        
        # 7. è®­ç»ƒæ¨¡å‹
        model = pipeline.fit(user_features)
        
        # 8. é¢„æµ‹
        predictions = model.transform(user_features)
        
        # 8.1 æ¨¡å‹è¯„ä¼°ï¼ˆä¿å­˜ç»“æœï¼Œç¨åæ‰“å°ï¼‰
        # è®¡ç®—AUC
        evaluator_auc = BinaryClassificationEvaluator(
            labelCol="is_high_risk",
            rawPredictionCol="rawPrediction",
            metricName="areaUnderROC"
        )
        auc = evaluator_auc.evaluate(predictions)
        
        # è®¡ç®—å‡†ç¡®ç‡ã€å¬å›ç‡ã€ç²¾ç¡®ç‡
        tp = predictions.filter((col("prediction") == 1.0) & (col("is_high_risk") == 1.0)).count()
        fp = predictions.filter((col("prediction") == 1.0) & (col("is_high_risk") == 0.0)).count()
        tn = predictions.filter((col("prediction") == 0.0) & (col("is_high_risk") == 0.0)).count()
        fn = predictions.filter((col("prediction") == 0.0) & (col("is_high_risk") == 1.0)).count()
        
        accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        self.evaluation_results["overdue_risk"] = {
            "auc": auc,
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }
        
        # 9. æå–é€¾æœŸæ¦‚ç‡ï¼ˆprobabilityåˆ—çš„ç¬¬äºŒä¸ªå…ƒç´ æ˜¯æ­£ç±»æ¦‚ç‡ï¼‰
        # åœ¨Driverç«¯ç”¨Pandasæå–Vectorç±»å‹çš„æ¦‚ç‡å€¼
        print("  æ­£åœ¨æå–é¢„æµ‹æ¦‚ç‡...")
        prob_pdf = predictions.select("userid", "probability").toPandas()
        prob_pdf["overdue_probability"] = prob_pdf["probability"].apply(
            lambda x: float(x[1]) if x is not None and len(x) > 1 else 0.0
        )
        
        # è½¬æ¢å›Spark DataFrame
        prob_schema = StructType([
            StructField("userid", StringType(), True),
            StructField("overdue_probability", DoubleType(), True)
        ])
        prob_df = self.spark.createDataFrame(
            prob_pdf[["userid", "overdue_probability"]].values.tolist(),
            schema=prob_schema
        )
        
        # åˆå¹¶æ¦‚ç‡æ•°æ®å›åŸDataFrameï¼Œå»é™¤MLä¸­é—´åˆ—
        result = predictions.drop("features_raw", "features", "rawPrediction", "probability", "prediction") \
            .join(prob_df, "userid", "inner")
        
        # 10. ç”Ÿæˆé£é™©ç­‰çº§
        result = result.withColumn(
            "risk_level",
            when(col("overdue_probability") >= 0.7, "é«˜é£é™©")
            .when(col("overdue_probability") >= 0.4, "ä¸­é£é™©")
            .when(col("overdue_probability") >= 0.2, "ä½é£é™©")
            .otherwise("æä½é£é™©")
        )
        
        # 11. ç”Ÿæˆé¢„è­¦å»ºè®®
        result = result.withColumn(
            "warning_message",
            when(col("risk_level") == "é«˜é£é™©", "å»ºè®®åŠ å¼ºé€¾æœŸæé†’ï¼Œè€ƒè™‘é™åˆ¶å€Ÿé˜…æ•°é‡")
            .when(col("risk_level") == "ä¸­é£é™©", "å»ºè®®å®šæœŸå‘é€è¿˜ä¹¦æé†’")
            .when(col("risk_level") == "ä½é£é™©", "æ­£å¸¸å€Ÿé˜…ï¼Œå¶å°”æé†’å³å¯")
            .otherwise("ä¼˜è´¨è¯»è€…ï¼Œæ— éœ€ç‰¹åˆ«å…³æ³¨")
        )
        
        # 12. è¾“å‡ºå­—æ®µï¼ˆä¸å†ä»user_summaryè·å–ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
        output = result.select(
            col("userid"),
            col("dept"),
            col("user_type"),
            col("historical_borrow_count").cast("long").alias("borrow_count"),
            spark_round(col("historical_overdue_rate"), 4).cast("double").alias("historical_overdue_rate"),
            spark_round(col("historical_avg_borrow_days"), 2).cast("double").alias("avg_borrow_days"),
            spark_round(col("overdue_probability"), 4).cast("double").alias("overdue_probability"),
            col("risk_level"),
            col("warning_message"),
            lit(self.latest_date).alias("prediction_date")
        )
        
        # 14. ä¿å­˜åˆ°Hive
        output.write \
            .mode("overwrite") \
            .format("parquet") \
            .saveAsTable("library_ads.ads_overdue_prediction")
        
        # 15. ä¿å­˜åˆ°MySQL
        output.write \
            .mode("overwrite") \
            .jdbc(self.mysql_url, "overdue_risk_prediction", properties=self.mysql_properties)
        
        # ç»Ÿè®¡é£é™©åˆ†å¸ƒ
        risk_stats = output.groupBy("risk_level").count().collect()
        print(f"âœ“ é€¾æœŸé£é™©é¢„æµ‹å®Œæˆ: {output.count():,} ä¸ªç”¨æˆ·")
        print("\n  é£é™©åˆ†å¸ƒï¼š")
        for row in risk_stats:
            print(f"    {row['risk_level']}: {row['count']:,}äºº")
        
        return output
    
    def predict_lend_trend(self):
        """
        å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹ - åŸºäºå†å²æ•°æ®é¢„æµ‹æœªæ¥å€Ÿé˜…é‡
        
        ä½¿ç”¨æœˆåº¦å€Ÿé˜…æ•°æ®ï¼Œé€šè¿‡å›å½’æ¨¡å‹é¢„æµ‹æœªæ¥è¶‹åŠ¿
        """
        print("\n" + "=" * 60)
        print("[2/3] å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹ï¼ˆæ—¶é—´åºåˆ—å›å½’ï¼‰...")
        
        # 1. æŒ‰æœˆç»Ÿè®¡å€Ÿé˜…é‡
        monthly_lend = self.lend_detail \
            .withColumn("lend_month", F.date_format(to_date(col("lend_date")), "yyyy-MM")) \
            .groupBy("lend_month") \
            .agg(
                count("*").alias("lend_count"),
                F.countDistinct("userid").alias("active_users"),
                F.countDistinct("book_id").alias("unique_books")
            ) \
            .orderBy("lend_month")
        
        # 2. æ·»åŠ æ—¶é—´ç‰¹å¾
        monthly_lend = monthly_lend \
            .withColumn("year", F.substring("lend_month", 1, 4).cast("int")) \
            .withColumn("month", F.substring("lend_month", 6, 2).cast("int"))
        
        # åˆ›å»ºæ—¶é—´ç´¢å¼•ï¼ˆä»ç¬¬ä¸€ä¸ªæœˆå¼€å§‹çš„æœˆæ•°ï¼‰
        min_year = monthly_lend.agg(spark_min("year")).collect()[0][0]
        monthly_lend = monthly_lend.withColumn(
            "month_index",
            ((col("year") - min_year) * 12 + col("month")).cast("double")
        )
        
        # æ·»åŠ å†å²è¶‹åŠ¿ç‰¹å¾ï¼ˆå‰1-3ä¸ªæœˆçš„å€Ÿé˜…é‡ï¼‰
        window = Window.orderBy("lend_month")
        monthly_lend = monthly_lend \
            .withColumn("prev_1_month", lag("lend_count", 1).over(window)) \
            .withColumn("prev_2_month", lag("lend_count", 2).over(window)) \
            .withColumn("prev_3_month", lag("lend_count", 3).over(window)) \
            .na.fill(0, ["prev_1_month", "prev_2_month", "prev_3_month"])
        
        # æ·»åŠ å­£èŠ‚æ€§ç‰¹å¾
        monthly_lend = monthly_lend \
            .withColumn("is_semester_start", when(col("month").isin([3, 9]), 1.0).otherwise(0.0)) \
            .withColumn("is_exam_period", when(col("month").isin([1, 6, 7, 12]), 1.0).otherwise(0.0)) \
            .withColumn("is_vacation", when(col("month").isin([2, 7, 8]), 1.0).otherwise(0.0))
        
        print(f"  å†å²æœˆä»½æ•°: {monthly_lend.count()}")
        
        # 3. å‡†å¤‡ç‰¹å¾ï¼ˆåŒ…å«å†å²è¶‹åŠ¿ï¼‰
        feature_cols = ["month_index", "month", "prev_1_month", "prev_2_month", "prev_3_month",
                        "is_semester_start", "is_exam_period", "is_vacation"]
        
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol="features"
        )
        
        # 4. éšæœºæ£®æ—å›å½’
        rf_regressor = RandomForestRegressor(
            featuresCol="features",
            labelCol="lend_count",
            predictionCol="predicted_count",
            numTrees=30,
            maxDepth=5,
            seed=42
        )
        
        pipeline = Pipeline(stages=[assembler, rf_regressor])
        
        # 5. è®­ç»ƒæ¨¡å‹
        model = pipeline.fit(monthly_lend)
        
        # 6. å¯¹å†å²æ•°æ®è¿›è¡Œæ‹Ÿåˆ
        fitted = model.transform(monthly_lend)
        
        # 6.1 æ¨¡å‹è¯„ä¼°ï¼ˆä¿å­˜ç»“æœï¼Œç¨åæ‰“å°ï¼‰
        # è®¡ç®—RMSEå’ŒRÂ²
        evaluator_rmse = RegressionEvaluator(
            labelCol="lend_count",
            predictionCol="predicted_count",
            metricName="rmse"
        )
        rmse = evaluator_rmse.evaluate(fitted)
        
        evaluator_r2 = RegressionEvaluator(
            labelCol="lend_count",
            predictionCol="predicted_count",
            metricName="r2"
        )
        r2 = evaluator_r2.evaluate(fitted)
        
        evaluator_mae = RegressionEvaluator(
            labelCol="lend_count",
            predictionCol="predicted_count",
            metricName="mae"
        )
        mae = evaluator_mae.evaluate(fitted)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        self.evaluation_results["lend_trend"] = {
            "rmse": rmse,
            "r2": r2,
            "mae": mae
        }
        
        # 7. ç”Ÿæˆæœªæ¥6ä¸ªæœˆçš„é¢„æµ‹ï¼ˆæ»šåŠ¨é¢„æµ‹ï¼‰
        # è·å–æœ€åä¸€ä¸ªæœˆçš„ä¿¡æ¯
        last_row = monthly_lend.orderBy(col("month_index").desc()).first()
        last_month_index = last_row["month_index"]
        last_year = last_row["year"]
        last_month = last_row["month"]
        
        # è·å–æœ€è¿‘3ä¸ªæœˆçš„å€Ÿé˜…é‡ï¼ˆç”¨äºç¬¬ä¸€ä¸ªæœˆé¢„æµ‹ï¼‰
        recent_3_months = monthly_lend.orderBy(col("month_index").desc()).limit(3).collect()
        prev_counts = [row["lend_count"] for row in reversed(recent_3_months)]
        while len(prev_counts) < 3:
            prev_counts.insert(0, 0)  # ä¸è¶³3ä¸ªæœˆæ—¶è¡¥0
        
        # æ»šåŠ¨é¢„æµ‹ï¼šæ¯æ¬¡é¢„æµ‹ä½¿ç”¨å‰ä¸€æ¬¡çš„é¢„æµ‹ç»“æœ
        future_results = []
        current_year = last_year
        current_month = last_month
        predicted_values = []  # å­˜å‚¨é¢„æµ‹å€¼ç”¨äºä¸‹ä¸€æ¬¡é¢„æµ‹
        
        for i in range(1, 7):  # é¢„æµ‹æœªæ¥6ä¸ªæœˆ
            current_month += 1
            if current_month > 12:
                current_month = 1
                current_year += 1
            
            # åŠ¨æ€æ›´æ–°å†å²çª—å£
            if i == 1:
                # ç¬¬ä¸€ä¸ªæœˆï¼šä½¿ç”¨çœŸå®å†å²
                p1, p2, p3 = prev_counts[-1], prev_counts[-2], prev_counts[-3]
            elif i == 2:
                # ç¬¬äºŒä¸ªæœˆï¼šprev_1æ˜¯ç¬¬ä¸€ä¸ªæœˆçš„é¢„æµ‹å€¼
                p1, p2, p3 = predicted_values[0], prev_counts[-1], prev_counts[-2]
            elif i == 3:
                # ç¬¬ä¸‰ä¸ªæœˆï¼šprev_1å’Œprev_2æ˜¯é¢„æµ‹å€¼
                p1, p2, p3 = predicted_values[1], predicted_values[0], prev_counts[-1]
            else:
                # ç¬¬å››ä¸ªæœˆåŠä»¥åï¼šå…¨éƒ¨ä½¿ç”¨é¢„æµ‹å€¼
                p1, p2, p3 = predicted_values[i-2], predicted_values[i-3], predicted_values[i-4]
            
            # æ„å»ºå½“å‰æœˆä»½çš„ç‰¹å¾
            month_data = {
                "lend_month": f"{current_year}-{current_month:02d}",
                "year": current_year,
                "month": current_month,
                "month_index": float(last_month_index + i),
                "prev_1_month": float(p1),
                "prev_2_month": float(p2),
                "prev_3_month": float(p3),
                "is_semester_start": 1.0 if current_month in [3, 9] else 0.0,
                "is_exam_period": 1.0 if current_month in [1, 6, 7, 12] else 0.0,
                "is_vacation": 1.0 if current_month in [2, 7, 8] else 0.0,
                "lend_count": 0,  # å ä½
                "active_users": 0,
                "unique_books": 0
            }
            
            # é¢„æµ‹å½“å‰æœˆä»½
            temp_df = self.spark.createDataFrame([month_data])
            temp_pred_df = model.transform(temp_df)
            temp_pred = temp_pred_df.select("predicted_count").collect()[0][0]
            predicted_values.append(temp_pred)
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            future_results.append({
                "lend_month": month_data["lend_month"],
                "year": month_data["year"],
                "month": month_data["month"],
                "lend_count": 0,
                "active_users": 0,
                "unique_books": 0,
                "predicted_count": int(round(temp_pred)),
                "data_type": "é¢„æµ‹"
            })
        
        # 8. è½¬æ¢ä¸ºDataFrame
        future_df = self.spark.createDataFrame(future_results)
        
        # 9. åˆå¹¶å†å²å’Œé¢„æµ‹æ•°æ®
        historical = fitted.select(
            col("lend_month"),
            col("year").cast("int"),
            col("month").cast("int"),
            col("lend_count").cast("long"),
            col("active_users").cast("long"),
            col("unique_books").cast("long"),
            spark_round(col("predicted_count"), 0).cast("long").alias("predicted_count"),
            lit("å†å²").alias("data_type")
        )
        
        # future_dfå·²ç»åŒ…å«æ­£ç¡®çš„å­—æ®µ
        future = future_df.select(
            col("lend_month"),
            col("year").cast("int"),
            col("month").cast("int"),
            col("lend_count").cast("long"),
            col("active_users").cast("long"),
            col("unique_books").cast("long"),
            col("predicted_count").cast("long"),
            col("data_type")
        )
        
        result = historical.union(future).orderBy("lend_month")
        
        # æ·»åŠ è¶‹åŠ¿åˆ¤æ–­
        window = Window.orderBy("lend_month")
        result = result.withColumn(
            "prev_predicted",
            lag("predicted_count", 1).over(window)
        ).withColumn(
            "trend",
            when(col("prev_predicted").isNull(), "æŒå¹³")
            .when(col("predicted_count") > col("prev_predicted") * 1.1, "ä¸Šå‡")
            .when(col("predicted_count") < col("prev_predicted") * 0.9, "ä¸‹é™")
            .otherwise("æŒå¹³")
        ).drop("prev_predicted")
        
        # æ·»åŠ é¢„æµ‹æ—¥æœŸ
        result = result.withColumn("prediction_date", lit(self.latest_date))
        
        # 10. ä¿å­˜åˆ°Hive
        result.write \
            .mode("overwrite") \
            .format("parquet") \
            .saveAsTable("library_ads.ads_lend_trend_prediction")
        
        # 11. ä¿å­˜åˆ°MySQL
        result.write \
            .mode("overwrite") \
            .jdbc(self.mysql_url, "lend_trend_prediction", properties=self.mysql_properties)
        
        print(f"âœ“ å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹å®Œæˆ: {result.count()} ä¸ªæœˆä»½ï¼ˆå«6ä¸ªæœˆé¢„æµ‹ï¼‰")
        
        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        print("\n  æœªæ¥6ä¸ªæœˆé¢„æµ‹ï¼š")
        future_result = result.filter(col("data_type") == "é¢„æµ‹").collect()
        for row in future_result:
            print(f"    {row['lend_month']}: é¢„æµ‹å€Ÿé˜…é‡ {row['predicted_count']:,} ({row['trend']})")
        
        return result
    
    def predict_book_heat(self):
        """
        å›¾ä¹¦çƒ­åº¦é¢„æµ‹ - é¢„æµ‹å›¾ä¹¦æœªæ¥çš„å€Ÿé˜…çƒ­åº¦
        
        æ ¸å¿ƒæ€è·¯ï¼šç”¨å†å²æœŸå’Œå‰æœŸç‰¹å¾é¢„æµ‹è¿‘æœŸçƒ­åº¦
        
        æ—¶é—´åˆ’åˆ†ï¼š
        - æ›´æ—©æœŸï¼ˆç‰¹å¾ï¼‰ï¼š6ä¸ªæœˆå‰ä»¥å‰
        - å‰æœŸï¼ˆç‰¹å¾ï¼‰ï¼š3-6ä¸ªæœˆå‰
        - è¿‘æœŸï¼ˆæ ‡ç­¾ï¼‰ï¼šæœ€è¿‘3ä¸ªæœˆ
        
        æ³¨æ„ï¼šè¿™æ˜¯å›æµ‹ï¼ˆBacktestingï¼‰æ¨¡å¼
        - è®­ç»ƒé›† = é¢„æµ‹é›†ï¼ˆç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼‰
        - è¾“å‡ºçš„é¢„æµ‹ç»“æœæ˜¯å¯¹"å·²çŸ¥ç»“æœ"çš„é¢„æµ‹
        - å®é™…åº”ç”¨æ—¶ï¼Œåº”è¯¥ç”¨å…¨éƒ¨å†å²æ•°æ®è®­ç»ƒï¼Œå¯¹å½“å‰å›¾ä¹¦é¢„æµ‹æœªæ¥çƒ­åº¦
        
        ç‰¹å¾ä¸åŒ…å«è¿‘æœŸæ•°æ®ï¼Œé¿å…æ•°æ®æ³„éœ²
        """
        print("\n" + "=" * 60)
        print("[3/3] å›¾ä¹¦çƒ­åº¦é¢„æµ‹ï¼ˆéšæœºæ£®æ—å›å½’ï¼‰...")
        
        # 1. è®¡ç®—å›¾ä¹¦ç‰¹å¾
        # å°†æ•°æ®åˆ†ä¸ºå‰æœŸï¼ˆç‰¹å¾ï¼‰å’Œè¿‘æœŸï¼ˆæ ‡ç­¾ï¼‰
        recent_date = (datetime.strptime(self.latest_date, "%Y-%m-%d") - timedelta(days=90)).strftime("%Y-%m-%d")
        early_date = (datetime.strptime(self.latest_date, "%Y-%m-%d") - timedelta(days=180)).strftime("%Y-%m-%d")
        print(f"  æ—¶é—´çª—å£: å†å²(<{early_date}) | å‰æœŸ({early_date}~{recent_date}) | è¿‘æœŸ(>={recent_date})")
        
        # è¿‘æœŸå€Ÿé˜…é‡ï¼ˆæœ€è¿‘3ä¸ªæœˆï¼‰- ä½œä¸ºæ ‡ç­¾
        recent_lend = self.lend_detail \
            .filter(col("lend_date") >= to_date(lit(recent_date))) \
            .groupBy("book_id") \
            .agg(
                count("*").alias("recent_lend_count"),
                F.countDistinct("userid").alias("recent_user_count")
            )
        
        # å‰æœŸå€Ÿé˜…é‡ï¼ˆ3-6ä¸ªæœˆå‰ï¼‰- ä½œä¸ºç‰¹å¾
        early_lend = self.lend_detail \
            .filter((col("lend_date") >= to_date(lit(early_date))) & (col("lend_date") < to_date(lit(recent_date)))) \
            .groupBy("book_id") \
            .agg(
                count("*").alias("early_lend_count"),
                F.countDistinct("userid").alias("early_user_count")
            )
        
        # æ›´æ—©æœŸå€Ÿé˜…é‡ï¼ˆ6ä¸ªæœˆå‰ä»¥å‰ï¼‰- è®¡ç®—æ‰€æœ‰ç‰¹å¾
        very_early_lend = self.lend_detail \
            .filter(col("lend_date") < to_date(lit(early_date))) \
            .groupBy("book_id") \
            .agg(
                count("*").alias("very_early_lend_count"),
                F.countDistinct("userid").alias("very_early_user_count"),
                avg("borrow_days").alias("very_early_avg_borrow_days"),
                spark_sum(when(col("renew_times") > 0, 1).otherwise(0)).alias("very_early_renew_count")
            )
        
        # åˆå¹¶å›¾ä¹¦åŸºæœ¬ä¿¡æ¯ï¼ˆåªä½¿ç”¨6ä¸ªæœˆå‰ä»¥å‰çš„æ•°æ®ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
        book_features = self.book_info.select("book_id", "title", "subject", "author") \
            .join(very_early_lend, "book_id", "left") \
            .join(early_lend, "book_id", "left") \
            .join(recent_lend, "book_id", "left") \
            .select(
                col("book_id"),
                col("title"),
                col("subject"),
                col("author"),
                F.coalesce(col("very_early_avg_borrow_days"), lit(0)).cast("double").alias("avg_borrow_days"),
                F.coalesce(col("very_early_renew_count"), lit(0)).cast("double").alias("renew_count"),
                F.coalesce(col("very_early_lend_count"), lit(0)).cast("double").alias("very_early_lend_count"),
                F.coalesce(col("very_early_user_count"), lit(0)).cast("double").alias("very_early_user_count"),
                F.coalesce(col("early_lend_count"), lit(0)).cast("double").alias("early_lend_count"),
                F.coalesce(col("early_user_count"), lit(0)).cast("double").alias("early_user_count"),
                F.coalesce(col("recent_lend_count"), lit(0)).cast("double").alias("recent_lend_count"),
                F.coalesce(col("recent_user_count"), lit(0)).cast("double").alias("recent_user_count")
            ) \
            .na.fill(0)
        
        # è®¡ç®—è¶‹åŠ¿ç‰¹å¾ï¼ˆå‰æœŸç›¸å¯¹äºæ›´æ—©æœŸçš„å¢é•¿ç‡ï¼‰
        book_features = book_features.withColumn(
            "early_growth_rate",
            when(col("very_early_lend_count") > 0, 
                 col("early_lend_count") / col("very_early_lend_count"))
            .otherwise(when(col("early_lend_count") > 0, 2.0).otherwise(0.0))  # æ–°ä¹¦é»˜è®¤å¢é•¿ç‡2.0
        )
        
        # è¿‡æ»¤æœ‰å€Ÿé˜…è®°å½•çš„å›¾ä¹¦ï¼ˆ6ä¸ªæœˆå‰ä»¥å‰æˆ–å‰æœŸæœ‰æ•°æ®ï¼‰
        book_features = book_features.filter(
            (col("very_early_lend_count") > 0) | (col("early_lend_count") > 0)
        )
        
        print(f"  å›¾ä¹¦æ ·æœ¬æ•°: {book_features.count():,}")
        
        # 2. ç‰¹å¾å·¥ç¨‹ - åªä½¿ç”¨6ä¸ªæœˆå‰ä»¥å‰å’Œå‰æœŸæ•°æ®ï¼Œä¸åŒ…å«è¿‘æœŸæ•°æ®
        feature_cols = ["very_early_lend_count", "very_early_user_count", 
                        "avg_borrow_days", "renew_count", 
                        "early_lend_count", "early_user_count", "early_growth_rate"]
        
        assembler = VectorAssembler(
            inputCols=feature_cols,
            outputCol="features"
        )
        
        # 3. ä½¿ç”¨è¿‘æœŸå€Ÿé˜…é‡ä½œä¸ºç›®æ ‡å˜é‡ï¼ˆç‰¹å¾ä¸åŒ…å«è¿‘æœŸæ•°æ®ï¼‰
        rf_regressor = RandomForestRegressor(
            featuresCol="features",
            labelCol="recent_lend_count",
            predictionCol="predicted_heat",
            numTrees=30,
            maxDepth=5,
            seed=42
        )
        
        pipeline = Pipeline(stages=[assembler, rf_regressor])
        
        # 4. è®­ç»ƒæ¨¡å‹
        model = pipeline.fit(book_features)
        
        # 5. é¢„æµ‹
        predictions = model.transform(book_features)
        
        # 5.1 æ¨¡å‹è¯„ä¼°ï¼ˆä¿å­˜ç»“æœï¼Œç¨åæ‰“å°ï¼‰
        # è®¡ç®—RMSEå’ŒRÂ²
        evaluator_rmse = RegressionEvaluator(
            labelCol="recent_lend_count",
            predictionCol="predicted_heat",
            metricName="rmse"
        )
        rmse = evaluator_rmse.evaluate(predictions)
        
        evaluator_r2 = RegressionEvaluator(
            labelCol="recent_lend_count",
            predictionCol="predicted_heat",
            metricName="r2"
        )
        r2 = evaluator_r2.evaluate(predictions)
        
        evaluator_mae = RegressionEvaluator(
            labelCol="recent_lend_count",
            predictionCol="predicted_heat",
            metricName="mae"
        )
        mae = evaluator_mae.evaluate(predictions)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        self.evaluation_results["book_heat"] = {
            "rmse": rmse,
            "r2": r2,
            "mae": mae
        }
        
        # 6. è®¡ç®—çƒ­åº¦åˆ†æ•°ï¼ˆå½’ä¸€åŒ–åˆ°0-100ï¼‰
        max_heat = predictions.agg(spark_max("predicted_heat")).collect()[0][0]
        min_heat = predictions.agg(spark_min("predicted_heat")).collect()[0][0]
        
        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼šçƒ­åº¦å€¼ç›¸åŒæˆ–ä¸ºç©ºæ—¶è®¾ç½®é»˜è®¤åˆ†æ•°
        if max_heat == min_heat or max_heat is None or min_heat is None:
            predictions = predictions.withColumn("heat_score", lit(50.0))
        else:
            predictions = predictions.withColumn(
                "heat_score",
                spark_round((col("predicted_heat") - lit(min_heat)) / lit(max_heat - min_heat) * 100, 2)
            )
        
        # 7. ç”Ÿæˆçƒ­åº¦ç­‰çº§
        predictions = predictions.withColumn(
            "heat_level",
            when(col("heat_score") >= 80, "çˆ†æ¬¾")
            .when(col("heat_score") >= 60, "çƒ­é—¨")
            .when(col("heat_score") >= 40, "ä¸€èˆ¬")
            .when(col("heat_score") >= 20, "å†·é—¨")
            .otherwise("æå†·")
        )
        
        # 8. ç”Ÿæˆè¶‹åŠ¿åˆ¤æ–­ï¼ˆæ¯”è¾ƒè¿‘æœŸä¸å‰æœŸï¼‰
        predictions = predictions.withColumn(
            "trend",
            when((col("early_lend_count") == 0) & (col("recent_lend_count") > 0), "ä¸Šå‡")  # æ–°ä¹¦æˆ–å†·é—¨ä¹¦çªç„¶çƒ­é—¨
            .when((col("early_lend_count") > 0) & (col("recent_lend_count") == 0), "ä¸‹é™")  # çƒ­é—¨ä¹¦å˜å†·é—¨
            .when(col("early_lend_count") == 0, "ç¨³å®š")  # ä¸¤æœŸéƒ½æ— æ•°æ®
            .when(col("recent_lend_count") > col("early_lend_count") * 1.2, "ä¸Šå‡")
            .when(col("recent_lend_count") < col("early_lend_count") * 0.8, "ä¸‹é™")
            .otherwise("ç¨³å®š")
        )
        
        # 9. ç”Ÿæˆé‡‡è´­å»ºè®®
        predictions = predictions.withColumn(
            "recommendation",
            when((col("heat_level") == "çˆ†æ¬¾") & (col("trend") == "ä¸Šå‡"), "å¼ºçƒˆå»ºè®®å¢åŠ é¦†è—")
            .when((col("heat_level") == "çƒ­é—¨") & (col("trend") == "ä¸Šå‡"), "å»ºè®®é€‚å½“å¢åŠ é¦†è—")
            .when((col("heat_level").isin("çˆ†æ¬¾", "çƒ­é—¨")) & (col("trend") == "ç¨³å®š"), "ç»´æŒç°æœ‰é¦†è—")
            .when(col("heat_level") == "æå†·", "è€ƒè™‘ä¸‹æ¶æˆ–å‰”æ—§")
            .otherwise("æ­£å¸¸ç®¡ç†")
        )
        
        # 10. é€‰æ‹©è¾“å‡ºå­—æ®µï¼ˆä½¿ç”¨6ä¸ªæœˆå‰ä»¥å‰+å‰æœŸçš„æ•°æ®ä½œä¸ºæ€»å€Ÿé˜…é‡ï¼‰
        output = predictions.select(
            col("book_id"),
            col("title"),
            col("subject"),
            col("author"),
            (col("very_early_lend_count") + col("early_lend_count")).cast("long").alias("total_lend_count"),
            col("recent_lend_count").cast("long").alias("recent_lend_count"),
            (col("very_early_user_count") + col("early_user_count")).cast("long").alias("unique_user_count"),
            spark_round(col("heat_score"), 2).cast("double").alias("heat_score"),
            col("heat_level"),
            col("trend"),
            col("recommendation"),
            lit(self.latest_date).alias("prediction_date")
        ).orderBy(col("heat_score").desc())
        
        # 11. ä¿å­˜åˆ°Hive
        output.write \
            .mode("overwrite") \
            .format("parquet") \
            .saveAsTable("library_ads.ads_book_heat_prediction")
        
        # 12. ä¿å­˜åˆ°MySQL
        output.write \
            .mode("overwrite") \
            .jdbc(self.mysql_url, "book_heat_prediction", properties=self.mysql_properties)
        
        # ç»Ÿè®¡çƒ­åº¦åˆ†å¸ƒ
        heat_stats = output.groupBy("heat_level").count().orderBy(col("count").desc()).collect()
        print(f"âœ“ å›¾ä¹¦çƒ­åº¦é¢„æµ‹å®Œæˆ: {output.count():,} æœ¬å›¾ä¹¦")
        print("\n  çƒ­åº¦åˆ†å¸ƒï¼š")
        for row in heat_stats:
            print(f"    {row['heat_level']}: {row['count']:,}æœ¬")
        
        # æ˜¾ç¤ºTOP10çƒ­é—¨å›¾ä¹¦
        print("\n  é¢„æµ‹çƒ­é—¨TOP10ï¼š")
        top10 = output.limit(10).collect()
        for i, row in enumerate(top10, 1):
            title = row['title'] if row['title'] else "æœªçŸ¥ä¹¦å"
            title_display = title[:20] if len(title) > 20 else title
            print(f"    {i}. ã€Š{title_display}ã€‹ çƒ­åº¦:{row['heat_score']:.1f} ({row['trend']})")
        
        return output
    
    def run(self):
        """è¿è¡Œæ‰€æœ‰é¢„æµ‹æ¨¡å‹"""
        print("\n" + "â–ˆ" * 60)
        print("å¼€å§‹é¢„æµ‹æ¨¡å‹åˆ†æ")
        print("â–ˆ" * 60)
        
        try:
            # åŠ è½½æ•°æ®
            self.load_data()
            
            # 1. é€¾æœŸé£é™©é¢„æµ‹
            self.predict_overdue_risk()
            
            # 2. å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹
            self.predict_lend_trend()
            
            # 3. å›¾ä¹¦çƒ­åº¦é¢„æµ‹
            self.predict_book_heat()
            
            print("\n" + "â–ˆ" * 60)
            print("âœ… é¢„æµ‹æ¨¡å‹åˆ†æå®Œæˆ")
            print("â–ˆ" * 60)
            print("ç”Ÿæˆçš„é¢„æµ‹è¡¨ï¼š")
            print("  1. ads_overdue_prediction      - ç”¨æˆ·é€¾æœŸé£é™©é¢„æµ‹")
            print("  2. ads_lend_trend_prediction   - å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹")
            print("  3. ads_book_heat_prediction    - å›¾ä¹¦çƒ­åº¦é¢„æµ‹")
            print("\nMySQLè¡¨ï¼š")
            print("  1. overdue_risk_prediction  - ç”¨æˆ·é€¾æœŸé£é™©")
            print("  2. lend_trend_prediction    - å€Ÿé˜…è¶‹åŠ¿")
            print("  3. book_heat_prediction     - å›¾ä¹¦çƒ­åº¦")
            
            # æ‰“å°æ¨¡å‹è¯„ä¼°ç»“æœ
            print("\n" + "â–ˆ" * 60)
            print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœæ±‡æ€»")
            print("â–ˆ" * 60)
            
            # é€¾æœŸé£é™©é¢„æµ‹è¯„ä¼°
            if self.evaluation_results["overdue_risk"]:
                print("\n[1] é€¾æœŸé£é™©é¢„æµ‹æ¨¡å‹ï¼ˆéšæœºæ£®æ—åˆ†ç±»å™¨ï¼‰")
                print("=" * 60)
                eval_data = self.evaluation_results["overdue_risk"]
                print(f"  AUC (ROCæ›²çº¿ä¸‹é¢ç§¯):  {eval_data['auc']:.4f}")
                print(f"  å‡†ç¡®ç‡ (Accuracy):     {eval_data['accuracy']:.4f}")
                print(f"  ç²¾ç¡®ç‡ (Precision):    {eval_data['precision']:.4f}")
                print(f"  å¬å›ç‡ (Recall):       {eval_data['recall']:.4f}")
                print(f"  F1åˆ†æ•°:                {eval_data['f1']:.4f}")
                
                # æ ¹æ®æŒ‡æ ‡ç»™å‡ºè¯„ä»·
                if eval_data['auc'] >= 0.8:
                    print(f"\n  âœ“ AUC={eval_data['auc']:.2f} è¡¨æ˜æ¨¡å‹å…·æœ‰è‰¯å¥½çš„åˆ†ç±»èƒ½åŠ›")
                else:
                    print(f"\n  âš  AUC={eval_data['auc']:.2f} æ¨¡å‹åˆ†ç±»èƒ½åŠ›ä¸€èˆ¬")
                
                if eval_data['recall'] >= 0.7:
                    print(f"  âœ“ å¬å›ç‡={eval_data['recall']*100:.1f}% èƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ«æ½œåœ¨é€¾æœŸç”¨æˆ·")
                elif eval_data['recall'] >= 0.5:
                    print(f"  âš  å¬å›ç‡={eval_data['recall']*100:.1f}% è¯†åˆ«èƒ½åŠ›ä¸­ç­‰ï¼Œå»ºè®®è°ƒæ•´é˜ˆå€¼")
                else:
                    print(f"  âš  å¬å›ç‡={eval_data['recall']*100:.1f}% è¾ƒä½ï¼Œæ¨¡å‹è¿‡äºä¿å®ˆ")
                    print(f"     å»ºè®®ï¼š1) ä½¿ç”¨æ ·æœ¬æƒé‡å¹³è¡¡æ•°æ® 2) è°ƒæ•´åˆ†ç±»é˜ˆå€¼ 3) å¢åŠ ç‰¹å¾")
                
                if eval_data['f1'] < 0.3:
                    print(f"  âš  F1={eval_data['f1']:.2f} è¾ƒä½ï¼Œç²¾ç¡®ç‡å’Œå¬å›ç‡ä¸å¹³è¡¡")
            
            # å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹è¯„ä¼°
            if self.evaluation_results["lend_trend"]:
                print("\n[2] å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹æ¨¡å‹ï¼ˆéšæœºæ£®æ—å›å½’ï¼‰")
                print("=" * 60)
                eval_data = self.evaluation_results["lend_trend"]
                print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE):     {eval_data['rmse']:.2f}")
                print(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE):    {eval_data['mae']:.2f}")
                print(f"  æ‹Ÿåˆä¼˜åº¦ (RÂ²):         {eval_data['r2']:.4f}")
                print(f"\n  âœ“ RÂ²={eval_data['r2']:.2f} è¯´æ˜æ¨¡å‹èƒ½è§£é‡Š{eval_data['r2']*100:.0f}%çš„å€Ÿé˜…é‡æ³¢åŠ¨")
                print(f"  âœ“ RMSE={eval_data['rmse']:.1f} é¢„æµ‹è¯¯å·®åœ¨å¯æ¥å—èŒƒå›´å†…")
            
            # å›¾ä¹¦çƒ­åº¦é¢„æµ‹è¯„ä¼°
            if self.evaluation_results["book_heat"]:
                print("\n[3] å›¾ä¹¦çƒ­åº¦é¢„æµ‹æ¨¡å‹ï¼ˆéšæœºæ£®æ—å›å½’ï¼‰")
                print("=" * 60)
                eval_data = self.evaluation_results["book_heat"]
                print(f"  å‡æ–¹æ ¹è¯¯å·® (RMSE):     {eval_data['rmse']:.2f}")
                print(f"  å¹³å‡ç»å¯¹è¯¯å·® (MAE):    {eval_data['mae']:.2f}")
                print(f"  æ‹Ÿåˆä¼˜åº¦ (RÂ²):         {eval_data['r2']:.4f}")
                
                # æ ¹æ®RÂ²ç»™å‡ºè¯„ä»·
                if eval_data['r2'] >= 0.7:
                    print(f"\n  âœ“ RÂ²={eval_data['r2']:.2f} æ¨¡å‹æ‹Ÿåˆæ•ˆæœè‰¯å¥½")
                elif eval_data['r2'] >= 0.5:
                    print(f"\n  âš  RÂ²={eval_data['r2']:.2f} æ¨¡å‹æ‹Ÿåˆæ•ˆæœä¸€èˆ¬")
                else:
                    print(f"\n  âš  RÂ²={eval_data['r2']:.2f} æ¨¡å‹æ‹Ÿåˆæ•ˆæœè¾ƒå·®")
                    print(f"     åŸå› ï¼šå›¾ä¹¦å€Ÿé˜…æ•°æ®ç¨€ç–ï¼Œå¤§éƒ¨åˆ†å›¾ä¹¦å€Ÿé˜…é‡å¾ˆå°‘")
                    print(f"     å»ºè®®ï¼š1) åªé¢„æµ‹çƒ­é—¨å›¾ä¹¦ 2) ä½¿ç”¨åˆ†ç±»è€Œéå›å½’ 3) å¢åŠ ç‰¹å¾")
                
                print(f"  âœ“ MAE={eval_data['mae']:.1f} å¹³å‡é¢„æµ‹åå·®è¾ƒå°")
            
            print("\n" + "â–ˆ" * 60)
            if (self.evaluation_results["overdue_risk"].get("recall", 0) < 0.5 or 
                self.evaluation_results["book_heat"].get("r2", 0) < 0.5):
                print("âš ï¸  éƒ¨åˆ†æ¨¡å‹æ€§èƒ½éœ€è¦ä¼˜åŒ–ï¼Œä½†æ•´ä½“é¢„æµ‹åŠŸèƒ½æ­£å¸¸")
            else:
                print("âœ… æ‰€æœ‰é¢„æµ‹æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œæ€§èƒ½æŒ‡æ ‡ç¬¦åˆé¢„æœŸ")
            print("â–ˆ" * 60)
            
        except Exception as e:
            print(f"\nâŒ é¢„æµ‹åˆ†æå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
        finally:
            self.spark.stop()


if __name__ == "__main__":
    predictor = PredictionModels()
    predictor.run()
