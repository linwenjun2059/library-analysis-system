#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ­¥éª¤4ï¼šæ•°æ®å¯¼å‡ºï¼ˆHive â†’ MySQLï¼‰
ç»Ÿä¸€å¯¼å‡º22å¼ è¡¨åˆ°MySQLä¾›ä¸šåŠ¡ç³»ç»Ÿä½¿ç”¨

è¡¨åˆ†ç±»ï¼š
- ç»´åº¦è¡¨ï¼ˆ3å¼ ï¼‰ï¼šç”¨æˆ·ç»´åº¦ã€å›¾ä¹¦ç»´åº¦ã€è¿‘æœŸå€Ÿé˜…è®°å½•
- æ±‡æ€»è¡¨ï¼ˆ5å¼ ï¼‰ï¼šç”¨æˆ·æ±‡æ€»ã€å›¾ä¹¦æ±‡æ€»ã€é™¢ç³»æ±‡æ€»ã€ä¸»é¢˜æ±‡æ€»ã€æ¯æ—¥ç»Ÿè®¡
- èšåˆè¡¨ï¼ˆ5å¼ ï¼‰ï¼šçƒ­é—¨å›¾ä¹¦ã€æ´»è·ƒç”¨æˆ·ã€é™¢ç³»åå¥½ã€å€Ÿé˜…è¶‹åŠ¿ã€è¿è¥çœ‹æ¿
- åŠŸèƒ½è¡¨ï¼ˆ9å¼ ï¼‰ï¼šç”¨æˆ·ç”»åƒã€ä¸“ä¸šé˜…è¯»ã€é¦†è—åˆ©ç”¨ã€å‡ºç‰ˆç¤¾åˆ†æã€å‡ºç‰ˆå¹´ä»½åˆ†æã€é€¾æœŸåˆ†æã€æ—¶é—´åˆ†å¸ƒã€ç”¨æˆ·æ’åã€æ¨èåŸºç¡€è¡¨

æ³¨ï¼šæ¨èç»“æœè¡¨ï¼ˆ2å¼ MySQL + 1å¼ Hiveï¼‰ç”±05_book_recommend.pyå•ç‹¬å¯¼å‡º
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.window import Window
import sys

class DataExporter:
    """æ•°æ®å¯¼å‡ºï¼šHive â†’ MySQLï¼ˆ22å¼ è¡¨ï¼‰"""
    
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("Data Export - Hive to MySQL") \
            .enableHiveSupport() \
            .getOrCreate()
        
        # MySQLè¿æ¥é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæ”¯æŒçµæ´»é…ç½®ï¼‰
        import os
        mysql_host = os.getenv("MYSQL_HOST", "master")
        mysql_port = os.getenv("MYSQL_PORT", "3306")
        mysql_user = os.getenv("MYSQL_USER", "root")
        mysql_password = os.getenv("MYSQL_PASSWORD", "780122")
        mysql_database = os.getenv("MYSQL_DATABASE", "library_analysis")
        
        self.mysql_url = f"jdbc:mysql://{mysql_host}:{mysql_port}/{mysql_database}?useSSL=false&allowPublicKeyRetrieval=true&serverTimezone=Asia/Shanghai"
        self.mysql_properties = {
            "user": mysql_user,
            "password": mysql_password,
            "driver": "com.mysql.cj.jdbc.Driver"
        }
        
        print(f"MySQLè¿æ¥: {mysql_host}:{mysql_port}/{mysql_database}")
        
        self.exported_count = 0
    
    # =============================================
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šç»´åº¦è¡¨ï¼ˆ3å¼ ï¼‰
    # =============================================
    
    def export_dimension_tables(self):
        """å¯¼å‡ºç»´åº¦è¡¨ï¼ˆ3å¼ ï¼‰"""
        print("\n" + "â–ˆ" * 60)
        print("ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¼å‡ºç»´åº¦è¡¨ï¼ˆ3å¼ ï¼‰")
        print("â–ˆ" * 60)
        
        self._export_user_dimension()
        self._export_book_dimension()
        self._export_recent_lend_records()
    
    def _export_user_dimension(self):
        """å¯¼å‡ºç”¨æˆ·ç»´åº¦è¡¨"""
        print("\n" + "=" * 60)
        print(f"[{self.exported_count + 1}/22] å¯¼å‡ºç”¨æˆ·ç»´åº¦è¡¨...")
        
        user_df = self.spark.sql("""
            SELECT 
                userid, sex as gender, dept as dept_name, redr_type_name as user_type_name,
                CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-01') AS DATE) as import_date
            FROM library_dwd.dwd_user_info
        """)
        
        # è·å–æ¯ä¸ªç”¨æˆ·çš„æœ€æ–°è®°å½•
        latest_user = user_df.groupBy("userid").agg(max("import_date").alias("latest_date"))
        final_user = user_df.join(latest_user, 
            ["userid"], "inner"
        ).where(user_df.import_date == latest_user.latest_date
        ).select("userid", "gender", "dept_name", "user_type_name").distinct()
        
        count = final_user.count()
        print(f"   ğŸ“Š ç”¨æˆ·æ•°é‡: {count:,}")
        
        final_user.write.mode("overwrite").jdbc(self.mysql_url, "user_dimension", properties=self.mysql_properties)
        print("   âœ… user_dimension")
        self.exported_count += 1
    
    def _export_book_dimension(self):
        """å¯¼å‡ºå›¾ä¹¦ç»´åº¦è¡¨"""
        print("\n" + "=" * 60)
        print(f"[{self.exported_count + 1}/22] å¯¼å‡ºå›¾ä¹¦ç»´åº¦è¡¨...")
        
        book_df = self.spark.sql("""
            SELECT book_id, title, author, publisher, isbn, pub_year, subject, 
                   call_no, location_name, doc_type_name,
                   CAST(CONCAT(year, '-', LPAD(month, 2, '0'), '-01') AS DATE) as import_date
            FROM library_dwd.dwd_book_info
        """)
        
        latest_book = book_df.groupBy("book_id").agg(max("import_date").alias("latest_date"))
        final_book = book_df.join(latest_book,
            ["book_id"], "inner"
        ).where(book_df.import_date == latest_book.latest_date
        ).select("book_id", "title", "author", "publisher",
                 "isbn", "pub_year", "subject", "call_no",
                 "location_name", "doc_type_name").distinct()
        
        count = final_book.count()
        print(f"   ğŸ“Š å›¾ä¹¦æ•°é‡: {count:,}")
        
        final_book.write.mode("overwrite").jdbc(self.mysql_url, "book_dimension", properties=self.mysql_properties)
        print("   âœ… book_dimension")
        self.exported_count += 1
    
    def _export_recent_lend_records(self):
        """å¯¼å‡ºè¿‘æœŸå€Ÿé˜…è®°å½•ï¼ˆæ•°æ®ä¸­æœ€è¿‘6ä¸ªæœˆï¼‰"""
        print("\n" + "=" * 60)
        print(f"[{self.exported_count + 1}/22] å¯¼å‡ºè¿‘æœŸå€Ÿé˜…è®°å½•ï¼ˆæ•°æ®ä¸­æœ€è¿‘6ä¸ªæœˆï¼‰...")
        
        # åŠ¨æ€ç›¸å¯¹æ—¶é—´ï¼šåŸºäºæ•°æ®ä¸­çš„æœ€æ–°æ—¥æœŸå¾€å‰æ¨180å¤©
        recent_df = self.spark.sql("""
            WITH max_date AS (
                SELECT MAX(lend_date) as latest_date
                FROM library_dwd.dwd_lend_detail
            )
            SELECT lend_id, userid, book_id, lend_date, lend_time,
                   ret_date, ret_time, renew_times, borrow_days, is_overdue
            FROM library_dwd.dwd_lend_detail
            WHERE lend_date >= DATE_SUB((SELECT latest_date FROM max_date), 180)
        """)
        
        count = recent_df.count()
        print(f"   ğŸ“Š è¿‘æœŸå€Ÿé˜…æ•°: {count:,}")
        
        recent_df.write.mode("overwrite").jdbc(self.mysql_url, "recent_lend_records", properties=self.mysql_properties)
        print("   âœ… recent_lend_records")
        self.exported_count += 1
    
    # =============================================
    # ç¬¬äºŒéƒ¨åˆ†ï¼šæ±‡æ€»è¡¨ï¼ˆ5å¼ ï¼‰
    # =============================================
    
    def export_summary_tables(self):
        """å¯¼å‡ºæ±‡æ€»è¡¨ï¼ˆ5å¼ ï¼‰"""
        print("\n" + "â–ˆ" * 60)
        print("ç¬¬äºŒéƒ¨åˆ†ï¼šå¯¼å‡ºæ±‡æ€»è¡¨ï¼ˆ5å¼ ï¼‰")
        print("â–ˆ" * 60)
        
        self._export_table("user_lend_summary", "library_dws.dws_user_lend_summary", "ç”¨æˆ·å€Ÿé˜…æ±‡æ€»")
        self._export_table("book_lend_summary", "library_dws.dws_book_lend_summary", "å›¾ä¹¦å€Ÿé˜…æ±‡æ€»")
        self._export_table("dept_lend_summary", "library_dws.dws_dept_lend_summary", "é™¢ç³»å€Ÿé˜…æ±‡æ€»")
        self._export_table("subject_lend_summary", "library_dws.dws_subject_lend_summary", "ä¸»é¢˜åˆ†ç±»æ±‡æ€»")
        
        # æ¯æ—¥ç»Ÿè®¡ï¼ˆæ•°æ®ä¸­æœ€è¿‘1å¹´ï¼‰
        print("\n" + "=" * 60)
        print(f"[{self.exported_count + 1}/22] å¯¼å‡ºæ¯æ—¥ç»Ÿè®¡ï¼ˆæ•°æ®ä¸­æœ€è¿‘1å¹´ï¼‰...")
        
        # åŠ¨æ€ç›¸å¯¹æ—¶é—´ï¼šåŸºäºæ•°æ®ä¸­çš„æœ€æ–°æ—¥æœŸå¾€å‰æ¨365å¤©
        daily_df = self.spark.sql("""
            WITH max_date AS (
                SELECT MAX(stat_date) as latest_date
                FROM library_dws.dws_daily_stats
            )
            SELECT stat_date, lend_count, return_count, new_user_count,
                   active_user_count, overdue_count, avg_borrow_days
            FROM library_dws.dws_daily_stats
            WHERE stat_date >= DATE_SUB((SELECT latest_date FROM max_date), 365)
        """)
        
        count = daily_df.count()
        print(f"   ğŸ“Š ç»Ÿè®¡å¤©æ•°: {count:,}")
        
        daily_df.write.mode("overwrite").jdbc(self.mysql_url, "daily_stats", properties=self.mysql_properties)
        print("   âœ… daily_stats")
        self.exported_count += 1
    
    # =============================================
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šèšåˆè¡¨ï¼ˆ5å¼ ï¼‰
    # =============================================
    
    def export_aggregation_tables(self):
        """å¯¼å‡ºèšåˆè¡¨ï¼ˆ5å¼ ï¼‰"""
        print("\n" + "â–ˆ" * 60)
        print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¯¼å‡ºèšåˆè¡¨ï¼ˆ5å¼ ï¼‰")
        print("â–ˆ" * 60)
        
        self._export_table("hot_books", "library_ads.ads_hot_books", "çƒ­é—¨å›¾ä¹¦æ’è¡Œ")
        self._export_table("active_users", "library_ads.ads_active_users", "æ´»è·ƒç”¨æˆ·æ’è¡Œ")
        self._export_table("dept_preference", "library_ads.ads_dept_preference", "é™¢ç³»åå¥½åˆ†æ")
        self._export_table("lend_trend", "library_ads.ads_lend_trend", "å€Ÿé˜…è¶‹åŠ¿")
        self._export_table("operation_dashboard", "library_ads.ads_operation_dashboard", "è¿è¥çœ‹æ¿")
    
    # =============================================
    # ç¬¬å››éƒ¨åˆ†ï¼šåŠŸèƒ½è¡¨ï¼ˆ9å¼ ï¼‰
    # =============================================
    
    def export_feature_tables(self):
        """å¯¼å‡ºåŠŸèƒ½è¡¨ï¼ˆ9å¼ ï¼‰- æ”¯æŒé«˜çº§ç®¡ç†å‘˜ã€å›¾ä¹¦ç®¡ç†å‘˜ã€æ™®é€šç”¨æˆ·åŠŸèƒ½"""
        print("\n" + "â–ˆ" * 60)
        print("ç¬¬å››éƒ¨åˆ†ï¼šå¯¼å‡ºåŠŸèƒ½è¡¨ï¼ˆ9å¼ ï¼‰")
        print("â–ˆ" * 60)
        
        # é«˜çº§ç®¡ç†å‘˜åŠŸèƒ½è¡¨ï¼ˆ5å¼ ï¼‰
        self._export_user_profile()
        self._export_major_reading_profile()
        self._export_table("collection_utilization_analysis", "library_ads.ads_collection_utilization", "é¦†è—åˆ©ç”¨åˆ†æ")
        self._export_table("publisher_analysis", "library_ads.ads_publisher_analysis", "å‡ºç‰ˆç¤¾åˆ†æ")
        self._export_table("publish_year_analysis", "library_ads.ads_publish_year_analysis", "å‡ºç‰ˆå¹´ä»½åˆ†æ")
        
        # å›¾ä¹¦ç®¡ç†å‘˜åŠŸèƒ½è¡¨ï¼ˆ2å¼ ï¼‰
        self._export_table("overdue_analysis", "library_ads.ads_overdue_analysis", "é€¾æœŸåˆ†æ")
        self._export_table("time_distribution", "library_ads.ads_time_distribution", "æ—¶é—´åˆ†å¸ƒ")
        
        # æ™®é€šç”¨æˆ·åŠŸèƒ½è¡¨ï¼ˆ2å¼ ï¼‰
        self._export_table("user_ranking", "library_ads.ads_user_ranking", "ç”¨æˆ·æ’å")
        self._export_table("book_recommend_base", "library_ads.ads_book_recommend_base", "å›¾ä¹¦æ¨èåŸºç¡€è¡¨")
    
    def _export_user_profile(self):
        """å¯¼å‡ºç”¨æˆ·ç”»åƒè¡¨ï¼ˆéœ€è¦å¤„ç†ARRAYç±»å‹è½¬JSONï¼‰"""
        print("\n" + "=" * 60)
        print(f"[{self.exported_count + 1}/22] å¯¼å‡ºç”¨æˆ·ç”»åƒåˆ†æ...")
        
        # è¯»å–Hiveè¡¨ï¼Œå°†ARRAYè½¬ä¸ºJSONå­—ç¬¦ä¸²
        df = self.spark.sql("""
            SELECT 
                userid,
                user_type,
                dept,
                occupation,
                gender,
                age_group,
                borrow_level,
                total_borrow_count,
                reading_breadth,
                to_json(favorite_subjects) as favorite_subjects,
                to_json(favorite_locations) as favorite_locations,
                avg_borrow_days,
                overdue_rate,
                last_borrow_date,
                to_json(user_tags) as user_tags
            FROM library_ads.ads_user_profile
        """)
        
        count = df.count()
        print(f"   ğŸ“Š è®°å½•æ•°: {count:,}")
        
        df.write.mode("overwrite").jdbc(self.mysql_url, "user_profile", properties=self.mysql_properties)
        print(f"   âœ… user_profile")
        self.exported_count += 1
    
    def _export_major_reading_profile(self):
        """å¯¼å‡ºä¸“ä¸šé˜…è¯»ç‰¹å¾è¡¨ï¼ˆéœ€è¦å¤„ç†ARRAYç±»å‹è½¬JSONï¼‰"""
        print("\n" + "=" * 60)
        print(f"[{self.exported_count + 1}/22] å¯¼å‡ºä¸“ä¸šé˜…è¯»ç‰¹å¾...")
        
        # è¯»å–Hiveè¡¨ï¼Œå°†ARRAYè½¬ä¸ºJSONå­—ç¬¦ä¸²
        df = self.spark.sql("""
            SELECT 
                dept,
                occupation,
                student_count,
                total_borrow_count,
                avg_borrow_per_student,
                to_json(core_subjects) as core_subjects,
                to_json(cross_subjects) as cross_subjects,
                reading_breadth_score,
                to_json(popular_books) as popular_books
            FROM library_ads.ads_major_reading_profile
        """)
        
        count = df.count()
        print(f"   ğŸ“Š è®°å½•æ•°: {count:,}")
        
        df.write.mode("overwrite").jdbc(self.mysql_url, "major_reading_profile", properties=self.mysql_properties)
        print(f"   âœ… major_reading_profile")
        self.exported_count += 1
    
    def _export_table(self, mysql_table, hive_table, desc):
        """é€šç”¨å¯¼å‡ºæ–¹æ³•"""
        print("\n" + "=" * 60)
        print(f"[{self.exported_count + 1}/22] å¯¼å‡º{desc}...")
        
        df = self.spark.sql(f"SELECT * FROM {hive_table}")
        count = df.count()
        print(f"   ğŸ“Š è®°å½•æ•°: {count:,}")
        
        df.write.mode("overwrite").jdbc(self.mysql_url, mysql_table, properties=self.mysql_properties)
        print(f"   âœ… {mysql_table}")
        self.exported_count += 1
    
    # =============================================
    # ä¸»æµç¨‹
    # =============================================
    
    def run(self):
        """è¿è¡Œå®Œæ•´å¯¼å‡ºæµç¨‹"""
        print("\n" + "â–ˆ" * 60)
        print("ğŸš€ å¼€å§‹å¯¼å‡ºæ•°æ®åˆ°MySQL")
        print("â–ˆ" * 60)
        print("ğŸ“‹ æ€»è®¡ï¼š22å¼ è¡¨")
        print("   - ç»´åº¦è¡¨ï¼š3å¼ ï¼ˆç”¨æˆ·/å›¾ä¹¦/è¿‘æœŸå€Ÿé˜…ï¼‰")
        print("   - æ±‡æ€»è¡¨ï¼š5å¼ ï¼ˆç”¨æˆ·/å›¾ä¹¦/é™¢ç³»/ä¸»é¢˜/æ¯æ—¥ç»Ÿè®¡ï¼‰")
        print("   - èšåˆè¡¨ï¼š5å¼ ï¼ˆçƒ­é—¨å›¾ä¹¦/æ´»è·ƒç”¨æˆ·/é™¢ç³»åå¥½/å€Ÿé˜…è¶‹åŠ¿/è¿è¥çœ‹æ¿ï¼‰")
        print("   - åŠŸèƒ½è¡¨ï¼š9å¼ ï¼ˆç”¨æˆ·ç”»åƒ/ä¸“ä¸šé˜…è¯»/é€¾æœŸåˆ†æ/é¦†è—åˆ©ç”¨/æ—¶é—´åˆ†å¸ƒ/ç”¨æˆ·æ’å/å‡ºç‰ˆç¤¾åˆ†æ/å‡ºç‰ˆå¹´ä»½åˆ†æ/æ¨èåŸºç¡€ï¼‰")
        print("ğŸ’¡ æ¨èç»“æœè¡¨ï¼š2å¼ MySQL + 1å¼ Hiveç”±05_book_recommend.pyå•ç‹¬å¯¼å‡º")
        print("â–ˆ" * 60)
        
        try:
            import time
            start_time = time.time()
            
            # ç¬¬ä¸€éƒ¨åˆ†ï¼šç»´åº¦è¡¨
            self.export_dimension_tables()
            
            # ç¬¬äºŒéƒ¨åˆ†ï¼šæ±‡æ€»è¡¨
            self.export_summary_tables()
            
            # ç¬¬ä¸‰éƒ¨åˆ†ï¼šèšåˆè¡¨
            self.export_aggregation_tables()
            
            # ç¬¬å››éƒ¨åˆ†ï¼šæ¨èä¸åŠŸèƒ½è¡¨
            self.export_feature_tables()
            
            end_time = time.time()
            elapsed = end_time - start_time
            
            print("\n" + "â–ˆ" * 60)
            print(f"âœ… å¯¼å‡ºå®Œæˆï¼å…±å¯¼å‡º {self.exported_count} å¼ è¡¨")
            print(f"â±ï¸  è€—æ—¶: {elapsed:.2f} ç§’")
            print("â–ˆ" * 60)
            print("\nğŸ’¡ æç¤ºï¼š")
            print("   - ç»´åº¦è¡¨ï¼šç”¨äºç”¨æˆ·ç™»å½•ã€å›¾ä¹¦æœç´¢ç­‰åŸºç¡€ä¸šåŠ¡")
            print("   - æ±‡æ€»è¡¨ï¼šç”¨äºå¿«é€Ÿç»Ÿè®¡æŸ¥è¯¢å’Œæ•°æ®åˆ†æ")
            print("   - èšåˆè¡¨ï¼šç”¨äºDashboardå¯è§†åŒ–å±•ç¤º")
            print("   - åŠŸèƒ½è¡¨ï¼šæ”¯æŒä¸‰å¤§è§’è‰²åŠŸèƒ½ï¼ˆé«˜çº§ç®¡ç†å‘˜/å›¾ä¹¦ç®¡ç†å‘˜/æ™®é€šç”¨æˆ·ï¼‰")
            print("   - å‡ºç‰ˆåˆ†æè¡¨ï¼šä¼˜åŒ–é¡µé¢åŠ è½½é€Ÿåº¦")
            print("   - æ¨èè¡¨ï¼šç”±05_book_recommend.pyç”Ÿæˆï¼ˆ2å¼ ï¼šæ¨èä¸»è¡¨+ç»Ÿè®¡è¡¨ï¼‰")
            print("   - å†å²å€Ÿé˜…æ˜ç»†ä¿ç•™åœ¨Hiveï¼ŒæŒ‰éœ€æŸ¥è¯¢")
            print("â–ˆ" * 60)
            
        except Exception as e:
            print(f"\nâŒ å¯¼å‡ºå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            raise
        finally:
            self.spark.stop()

if __name__ == "__main__":
    exporter = DataExporter()
    exporter.run()
