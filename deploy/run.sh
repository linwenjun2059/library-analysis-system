#!/bin/bash
# =============================================
# å›¾ä¹¦é¦†æ•°æ®åˆ†æç³»ç»Ÿ - æ•°æ®é“¾è·¯æ‰§è¡Œè„šæœ¬
# ç”¨æ³•: bash run.sh [æ­¥éª¤å·]
#   ä¸å¸¦å‚æ•°: æ‰§è¡Œå…¨éƒ¨æ­¥éª¤
#   å¸¦å‚æ•°: æ‰§è¡ŒæŒ‡å®šæ­¥éª¤ (1-6)
# =============================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "${SCRIPT_DIR}/config.sh"

# =============================================
# æ­¥éª¤1: ä¸Šä¼ åŸå§‹æ•°æ®åˆ°HDFS
# =============================================
step1_upload_to_hdfs() {
    print_header "æ­¥éª¤1: ä¸Šä¼ åŸå§‹æ•°æ®åˆ°HDFS"
    
    print_step "1" "æ£€æŸ¥HDFSç›®å½•..."
    hdfs dfs -test -d ${HDFS_RAW_PATH}
    if [ $? -ne 0 ]; then
        print_step "1.1" "åˆ›å»ºHDFSç›®å½•: ${HDFS_RAW_PATH}"
        hdfs dfs -mkdir -p ${HDFS_RAW_PATH}
    fi
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    print_step "2" "æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨..."
    CSV_EXISTS=0
    for file in ${LOCAL_CSV_FILE}; do
        filename=$(basename "$file")
        hdfs dfs -test -e "${HDFS_RAW_PATH}/${filename}"
        if [ $? -eq 0 ]; then
            CSV_EXISTS=1
            print_warning "æ–‡ä»¶å·²å­˜åœ¨: ${filename}"
        fi
    done
    
    if [ $CSV_EXISTS -eq 1 ]; then
        echo ""
        echo " HDFSä¸Šå·²å­˜åœ¨CSVæ–‡ä»¶"
        read -p "æ˜¯å¦é‡æ–°ä¸Šä¼ ï¼Ÿ(yes/noï¼Œé»˜è®¤no): " reupload
        if [ "$reupload" != "yes" ]; then
            print_success "è·³è¿‡ä¸Šä¼ ï¼Œä½¿ç”¨å·²æœ‰æ–‡ä»¶"
            hdfs dfs -ls ${HDFS_RAW_PATH}
            return 0
        fi
        print_step "3" "é‡æ–°ä¸Šä¼ CSVæ–‡ä»¶..."
        hdfs dfs -put -f ${LOCAL_CSV_FILE} ${HDFS_RAW_PATH}/
    else
        print_step "3" "ä¸Šä¼ CSVæ–‡ä»¶åˆ°HDFS..."
        hdfs dfs -put ${LOCAL_CSV_FILE} ${HDFS_RAW_PATH}/
    fi
    
    if [ $? -eq 0 ]; then
        print_success "æ•°æ®ä¸Šä¼ å®Œæˆ"
        hdfs dfs -ls ${HDFS_RAW_PATH}
        return 0
    else
        print_error "æ•°æ®ä¸Šä¼ å¤±è´¥"
        return 1
    fi
}

# =============================================
# æ­¥éª¤2: åˆ›å»ºHiveè¡¨ç»“æ„
# =============================================
step2_create_hive_tables() {
    print_header "æ­¥éª¤2: åˆ›å»ºHiveè¡¨ç»“æ„"
    
    HIVE_SCRIPT_DIR="${SCRIPT_DIR}/../bigdata/hive"
    
    print_step "1" "åˆ›å»ºODSå±‚è¡¨ï¼ˆåŸå§‹æ•°æ®å±‚ï¼‰..."
    hive -S -f "${HIVE_SCRIPT_DIR}/01_create_ods.sql"
    if [ $? -ne 0 ]; then
        print_error "ODSå±‚è¡¨åˆ›å»ºå¤±è´¥"
        return 1
    fi
    
    print_step "2" "åˆ›å»ºDWDå±‚è¡¨ï¼ˆæ˜ç»†æ•°æ®å±‚ï¼‰..."
    hive -S -f "${HIVE_SCRIPT_DIR}/02_create_dwd.sql"
    if [ $? -ne 0 ]; then
        print_error "DWDå±‚è¡¨åˆ›å»ºå¤±è´¥"
        return 1
    fi
    
    print_step "3" "åˆ›å»ºDWSå±‚è¡¨ï¼ˆæ±‡æ€»æ•°æ®å±‚ï¼‰..."
    hive -S -f "${HIVE_SCRIPT_DIR}/03_create_dws.sql"
    if [ $? -ne 0 ]; then
        print_error "DWSå±‚è¡¨åˆ›å»ºå¤±è´¥"
        return 1
    fi
    
    print_step "4" "åˆ›å»ºADSå±‚è¡¨ï¼ˆåº”ç”¨æ•°æ®å±‚ï¼‰..."
    hive -S -f "${HIVE_SCRIPT_DIR}/04_create_ads.sql"
    if [ $? -ne 0 ]; then
        print_error "ADSå±‚è¡¨åˆ›å»ºå¤±è´¥"
        return 1
    fi
    
    print_success "Hiveè¡¨ç»“æ„åˆ›å»ºå®Œæˆï¼ˆODS/DWD/DWS/ADSï¼‰"
    return 0
}

# =============================================
# æ­¥éª¤3: Sparkæ•°æ®æ¸…æ´—ï¼ˆODS â†’ DWDï¼‰
# =============================================
step3_spark_clean() {
    print_header "æ­¥éª¤3: Sparkæ•°æ®æ¸…æ´—ï¼ˆODS â†’ DWDï¼‰"
    
    spark-submit \
      --master yarn \
      --deploy-mode client \
      --name "01_data_clean" \
      --num-executors ${SPARK_NUM_EXECUTORS} \
      --executor-memory ${SPARK_EXECUTOR_MEMORY} \
      --executor-cores ${SPARK_EXECUTOR_CORES} \
      --driver-memory ${SPARK_DRIVER_MEMORY} \
      --conf spark.dynamicAllocation.enabled=false \
      --conf spark.sql.shuffle.partitions=20 \
      --conf spark.sql.warehouse.dir=${SPARK_WAREHOUSE_DIR} \
      --conf spark.yarn.queue=default \
      --conf spark.sql.sources.partitionOverwriteMode=dynamic \
      ${PYTHON_SCRIPT_DIR}/01_data_clean.py all
    
    if [ $? -eq 0 ]; then
        print_success "æ•°æ®æ¸…æ´—å®Œæˆï¼ˆDWDå±‚ï¼‰"
        return 0
    else
        print_error "æ•°æ®æ¸…æ´—å¤±è´¥"
        return 1
    fi
}

# =============================================
# æ­¥éª¤4: Sparkæ•°æ®æ±‡æ€»ï¼ˆDWD â†’ DWSï¼‰
# =============================================
step4_spark_aggregate() {
    print_header "æ­¥éª¤4: Sparkæ•°æ®æ±‡æ€»ï¼ˆDWD â†’ DWSï¼‰"
    
    spark-submit \
      --master yarn \
      --deploy-mode client \
      --name "02_data_aggregate" \
      --num-executors ${SPARK_NUM_EXECUTORS} \
      --executor-memory ${SPARK_EXECUTOR_MEMORY} \
      --executor-cores ${SPARK_EXECUTOR_CORES} \
      --driver-memory ${SPARK_DRIVER_MEMORY} \
      --conf spark.dynamicAllocation.enabled=false \
      --conf spark.sql.shuffle.partitions=20 \
      --conf spark.sql.warehouse.dir=${SPARK_WAREHOUSE_DIR} \
      ${PYTHON_SCRIPT_DIR}/02_data_aggregate.py all
    
    if [ $? -eq 0 ]; then
        print_success "æ•°æ®æ±‡æ€»å®Œæˆï¼ˆDWSå±‚ - 5å¼ è¡¨ï¼‰"
        return 0
    else
        print_error "æ•°æ®æ±‡æ€»å¤±è´¥"
        return 1
    fi
}

# =============================================
# æ­¥éª¤5: Sparkæ•°æ®åˆ†æï¼ˆDWS â†’ ADSï¼‰
# =============================================
step5_spark_analyze() {
    print_header "æ­¥éª¤5: Sparkæ•°æ®åˆ†æï¼ˆDWS â†’ ADSï¼‰"
    
    spark-submit \
      --master yarn \
      --deploy-mode client \
      --name "03_data_analyze" \
      --num-executors ${SPARK_NUM_EXECUTORS} \
      --executor-memory ${SPARK_EXECUTOR_MEMORY} \
      --executor-cores ${SPARK_EXECUTOR_CORES} \
      --driver-memory ${SPARK_DRIVER_MEMORY} \
      --conf spark.dynamicAllocation.enabled=false \
      --conf spark.sql.shuffle.partitions=20 \
      --conf spark.sql.warehouse.dir=${SPARK_WAREHOUSE_DIR} \
      ${PYTHON_SCRIPT_DIR}/03_data_analyze.py
    
    if [ $? -eq 0 ]; then
        print_success "æ•°æ®åˆ†æå®Œæˆï¼ˆADSå±‚ - 12å¼ è¡¨ï¼‰"
        return 0
    else
        print_error "æ•°æ®åˆ†æå¤±è´¥"
        return 1
    fi
}

# =============================================
# æ­¥éª¤6: å¯¼å‡ºæ•°æ®åˆ°MySQLï¼ˆ20å¼ è¡¨ï¼‰
# =============================================
step6_export_mysql() {
    print_header "æ­¥éª¤6: å¯¼å‡ºæ•°æ®åˆ°MySQLï¼ˆ20å¼ è¡¨ï¼‰"
    
    # æ£€æŸ¥MySQL JDBCé©±åŠ¨
    if [ ! -f "${MYSQL_JDBC_JAR}" ]; then
        print_error "MySQL JDBCé©±åŠ¨ä¸å­˜åœ¨: ${MYSQL_JDBC_JAR}"
        print_warning "è¯·ä¸‹è½½å¹¶æ”¾ç½®åˆ°è¯¥è·¯å¾„ï¼Œæˆ–ä¿®æ”¹config.shä¸­çš„è·¯å¾„"
        return 1
    fi
    
    # ç¡®ä¿MySQLé…ç½®ç¯å¢ƒå˜é‡å·²å¯¼å‡ºï¼ˆconfig.shå·²å¯¼å‡ºï¼Œè¿™é‡Œå†æ¬¡ç¡®è®¤ï¼‰
    export MYSQL_HOST MYSQL_PORT MYSQL_USER MYSQL_PASSWORD MYSQL_DATABASE
    
    print_step "1" "MySQLè¿æ¥é…ç½®: ${MYSQL_HOST}:${MYSQL_PORT}/${MYSQL_DATABASE}"
    
    spark-submit \
      --master yarn \
      --deploy-mode client \
      --name "04_data_export" \
      --num-executors 3 \
      --executor-memory 2G \
      --executor-cores 2 \
      --driver-memory 2G \
      --jars ${MYSQL_JDBC_JAR} \
      --conf spark.dynamicAllocation.enabled=false \
      --conf spark.sql.shuffle.partitions=20 \
      --conf spark.sql.warehouse.dir=${SPARK_WAREHOUSE_DIR} \
      ${PYTHON_SCRIPT_DIR}/04_data_export.py
    
    if [ $? -eq 0 ]; then
        print_success "æ•°æ®å¯¼å‡ºå®Œæˆï¼ˆ20å¼ è¡¨ï¼‰"
        return 0
    else
        print_error "æ•°æ®å¯¼å‡ºå¤±è´¥"
        return 1
    fi
}

# =============================================
# æ­¥éª¤7: Sparkæ¨èç®—æ³•ï¼ˆå¯é€‰ï¼‰
# =============================================
step7_spark_recommend() {
    print_header "æ­¥éª¤7: Sparkæ¨èç®—æ³•"
    
    # æ£€æŸ¥MySQL JDBCé©±åŠ¨
    if [ ! -f "${MYSQL_JDBC_JAR}" ]; then
        print_error "MySQL JDBCé©±åŠ¨ä¸å­˜åœ¨: ${MYSQL_JDBC_JAR}"
        print_warning "è¯·ä¸‹è½½å¹¶æ”¾ç½®åˆ°è¯¥è·¯å¾„ï¼Œæˆ–ä¿®æ”¹config.shä¸­çš„è·¯å¾„"
        return 1
    fi
    
    spark-submit \
      --master yarn \
      --deploy-mode client \
      --name "05_book_recommend" \
      --num-executors 1 \
      --executor-memory 3g \
      --executor-cores 2 \
      --driver-memory 1g \
      --conf spark.sql.shuffle.partitions=30 \
      --jars ${MYSQL_JDBC_JAR} \
      ${PYTHON_SCRIPT_DIR}/05_book_recommend.py all 

    if [ $? -eq 0 ]; then
        print_success "æ¨èç®—æ³•å®Œæˆ"
        return 0
    else
        print_error "æ¨èç®—æ³•å¤±è´¥"
        return 1
    fi
}

# =============================================
# æ­¥éª¤8: é«˜çº§æ•°æ®æŒ–æ˜åˆ†æï¼ˆå…³è”è§„åˆ™+èšç±»ï¼‰
# =============================================
step8_spark_advanced() {
    print_header "æ­¥éª¤8: é«˜çº§æ•°æ®æŒ–æ˜åˆ†æï¼ˆFPGrowth+K-meansï¼‰"
    
    # æ£€æŸ¥MySQL JDBCé©±åŠ¨
    if [ ! -f "${MYSQL_JDBC_JAR}" ]; then
        print_error "MySQL JDBCé©±åŠ¨ä¸å­˜åœ¨: ${MYSQL_JDBC_JAR}"
        print_warning "è¯·ä¸‹è½½å¹¶æ”¾ç½®åˆ°è¯¥è·¯å¾„ï¼Œæˆ–ä¿®æ”¹config.shä¸­çš„è·¯å¾„"
        return 1
    fi
    
    spark-submit \
      --master yarn \
      --deploy-mode client \
      --name "06_advanced_analysis" \
      --num-executors 2 \
      --executor-memory 3g \
      --executor-cores 2 \
      --driver-memory 2g \
      --conf spark.sql.shuffle.partitions=20 \
      --jars ${MYSQL_JDBC_JAR} \
      ${PYTHON_SCRIPT_DIR}/06_advanced_analysis.py

    if [ $? -eq 0 ]; then
        print_success "é«˜çº§æ•°æ®æŒ–æ˜åˆ†æå®Œæˆï¼ˆå…³è”è§„åˆ™+ç”¨æˆ·èšç±»ï¼‰"
        return 0
    else
        print_error "é«˜çº§æ•°æ®æŒ–æ˜åˆ†æå¤±è´¥"
        return 1
    fi
}

# =============================================
# æ­¥éª¤9: é¢„æµ‹æ¨¡å‹åˆ†æï¼ˆé€¾æœŸé£é™©+è¶‹åŠ¿+çƒ­åº¦ï¼‰
# =============================================
step9_spark_prediction() {
    print_header "æ­¥éª¤9: é¢„æµ‹æ¨¡å‹åˆ†æï¼ˆéšæœºæ£®æ—é¢„æµ‹ï¼‰"
    
    # æ£€æŸ¥MySQL JDBCé©±åŠ¨
    if [ ! -f "${MYSQL_JDBC_JAR}" ]; then
        print_error "MySQL JDBCé©±åŠ¨ä¸å­˜åœ¨: ${MYSQL_JDBC_JAR}"
        print_warning "è¯·ä¸‹è½½å¹¶æ”¾ç½®åˆ°è¯¥è·¯å¾„ï¼Œæˆ–ä¿®æ”¹config.shä¸­çš„è·¯å¾„"
        return 1
    fi
    
    spark-submit \
      --master yarn \
      --deploy-mode client \
      --name "07_prediction_models" \
      --num-executors 2 \
      --executor-memory 3g \
      --executor-cores 2 \
      --driver-memory 2g \
      --conf spark.sql.shuffle.partitions=20 \
      --jars ${MYSQL_JDBC_JAR} \
      ${PYTHON_SCRIPT_DIR}/07_prediction_models.py

    if [ $? -eq 0 ]; then
        print_success "é¢„æµ‹æ¨¡å‹åˆ†æå®Œæˆï¼ˆé€¾æœŸé£é™©+å€Ÿé˜…è¶‹åŠ¿+å›¾ä¹¦çƒ­åº¦ï¼‰"
        return 0
    else
        print_error "é¢„æµ‹æ¨¡å‹åˆ†æå¤±è´¥"
        return 1
    fi
}

# =============================================
# æ‰§è¡Œå…¨éƒ¨æ­¥éª¤
# =============================================
run_all_steps() {
    print_header "å›¾ä¹¦é¦†æ•°æ®åˆ†æç³»ç»Ÿ - å®Œæ•´æ•°æ®é“¾è·¯"
    echo "åˆ†åŒºç­–ç•¥: year + month (å†å²æ•°æ® 2019-2020)"
    echo "MySQL JDBC: ${MYSQL_JDBC_JAR}"
    echo ""
    
    START_TIME=$(date +%s)
    
    step1_upload_to_hdfs || exit 1
    echo ""
    
    step2_create_hive_tables || exit 1
    echo ""
    
    step3_spark_clean || exit 1
    echo ""
    
    step4_spark_aggregate || exit 1
    echo ""
    
    step5_spark_analyze || exit 1
    echo ""
    
    step6_export_mysql || exit 1
    echo ""
    
    # æ¨èç®—æ³•æ˜¯å¯é€‰çš„
    if [ "${RUN_RECOMMEND:-yes}" = "yes" ]; then
        step7_spark_recommend || print_warning "æ¨èç®—æ³•å¤±è´¥ï¼ˆå¯é€‰æ­¥éª¤ï¼‰"
        echo ""
    fi
    
    # é«˜çº§æ•°æ®æŒ–æ˜åˆ†ææ˜¯å¯é€‰çš„
    if [ "${RUN_ADVANCED:-yes}" = "yes" ]; then
        step8_spark_advanced || print_warning "é«˜çº§æ•°æ®æŒ–æ˜åˆ†æå¤±è´¥ï¼ˆå¯é€‰æ­¥éª¤ï¼‰"
        echo ""
    fi
    
    # é¢„æµ‹æ¨¡å‹åˆ†ææ˜¯å¯é€‰çš„
    if [ "${RUN_PREDICTION:-yes}" = "yes" ]; then
        step9_spark_prediction || print_warning "é¢„æµ‹æ¨¡å‹åˆ†æå¤±è´¥ï¼ˆå¯é€‰æ­¥éª¤ï¼‰"
        echo ""
    fi
    
    END_TIME=$(date +%s)
    DURATION=$((END_TIME - START_TIME))
    MINUTES=$((DURATION / 60))
    SECONDS=$((DURATION % 60))
    
    print_header "âœ… å…¨éƒ¨æ­¥éª¤æ‰§è¡Œå®Œæˆï¼"
    echo "æ€»è€—æ—¶: ${MINUTES}åˆ†${SECONDS}ç§’"
    echo ""
    echo "éªŒè¯ç»“æœè¯·æ‰§è¡Œ: bash verify.sh"
}

# =============================================
# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
# =============================================
show_usage() {
    cat << EOF

ç”¨æ³•: bash run.sh [æ­¥éª¤å·]

ä¸å¸¦å‚æ•°: æ‰§è¡Œå…¨éƒ¨æ­¥éª¤ (1-9)
å¸¦å‚æ•°: æ‰§è¡ŒæŒ‡å®šæ­¥éª¤

ğŸ“‹ åˆ†å±‚æ•°ä»“æµç¨‹ï¼ˆ6æ­¥éª¤ + ç®—æ³•ï¼‰:
  1 - ä¸Šä¼ åŸå§‹æ•°æ®åˆ°HDFS
  2 - åˆ›å»ºHiveè¡¨ç»“æ„ (ODS/DWD/DWS/ADS)
  3 - æ•°æ®æ¸…æ´— (ODS â†’ DWD) 
       è„šæœ¬ï¼š01_data_clean.py
       è¾“å‡ºï¼š3å¼ ç»´åº¦è¡¨ï¼ˆç”¨æˆ·ã€å›¾ä¹¦ã€å€Ÿé˜…æ˜ç»†ï¼‰
  4 - æ•°æ®æ±‡æ€» (DWD â†’ DWS)
       è„šæœ¬ï¼š02_data_aggregate.py
       è¾“å‡ºï¼š5å¼ æ±‡æ€»è¡¨
  5 - æ•°æ®åˆ†æ (DWS â†’ ADS)
       è„šæœ¬ï¼š03_data_analyze.py
       è¾“å‡ºï¼š12å¼ åˆ†æè¡¨
  6 - å¯¼å‡ºMySQL (Hive â†’ MySQL)
       è„šæœ¬ï¼š04_data_export.py
       è¾“å‡ºï¼š23å¼ è¡¨ï¼ˆç»´åº¦3 + æ±‡æ€»5 + èšåˆ5 + åŠŸèƒ½10ï¼‰
  7 - æ¨èç®—æ³•ï¼ˆå¯é€‰ï¼‰
       è„šæœ¬ï¼š05_book_recommend.py
       è¾“å‡ºï¼šæ¨èè¡¨ï¼ˆALSååŒè¿‡æ»¤+å†…å®¹æ¨è+çƒ­é—¨æ¨èï¼‰
  8 - é«˜çº§æ•°æ®æŒ–æ˜ï¼ˆå¯é€‰ï¼‰
       è„šæœ¬ï¼š06_advanced_analysis.py
       è¾“å‡ºï¼šå…³è”è§„åˆ™ï¼ˆFPGrowthï¼‰+ ç”¨æˆ·èšç±»ï¼ˆK-meansï¼‰
  9 - é¢„æµ‹æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
       è„šæœ¬ï¼š07_prediction_models.py
       è¾“å‡ºï¼šé€¾æœŸé£é™©é¢„æµ‹ + å€Ÿé˜…è¶‹åŠ¿é¢„æµ‹ + å›¾ä¹¦çƒ­åº¦é¢„æµ‹

å®Œæ•´æ•°æ®æµç¨‹: 
  CSVæ–‡ä»¶ â†’ HDFS â†’ ODSå±‚ â†’ Sparkæ¸…æ´— â†’ DWDå±‚ï¼ˆæ˜ç»†æ•°æ®ï¼‰
  â†’ Sparkèšåˆ â†’ DWSå±‚ï¼ˆæ±‡æ€»ç»Ÿè®¡ï¼‰â†’ Sparkåˆ†æ â†’ ADSå±‚ï¼ˆåº”ç”¨ä¸»é¢˜ï¼‰
  â†’ MySQLå¯¼å‡º â†’ å‰ç«¯å±•ç¤º

æ³¨æ„: 
  - åˆ†åŒºç­–ç•¥: æŒ‰year+monthåŒå±‚åˆ†åŒºï¼ˆé€‚åˆ2019-2020å†å²æ•°æ®ï¼‰
  - å®Œæ•´åˆ†å±‚: ODS â†’ DWD â†’ DWS â†’ ADS â†’ MySQL

ç¤ºä¾‹:
  bash run.sh        # æ‰§è¡Œå…¨éƒ¨æ­¥éª¤
  bash run.sh 3      # åªæ‰§è¡Œæ­¥éª¤3
  bash run.sh 5      # åªæ‰§è¡Œæ­¥éª¤5

éªŒè¯:
  bash verify.sh     # éªŒè¯æ‰€æœ‰æ­¥éª¤
  bash verify.sh 3   # éªŒè¯æ­¥éª¤3

EOF
}

# =============================================
# ä¸»å‡½æ•°
# =============================================
main() {
    STEP=$1
    
    if [ -z "$STEP" ]; then
        # ä¸å¸¦å‚æ•°ï¼Œæ‰§è¡Œå…¨éƒ¨æ­¥éª¤
        run_all_steps
    elif [ "$STEP" = "-h" ] || [ "$STEP" = "--help" ]; then
        # æ˜¾ç¤ºå¸®åŠ©
        show_usage
    else
        # æ‰§è¡ŒæŒ‡å®šæ­¥éª¤
        case $STEP in
            1)
                step1_upload_to_hdfs
                ;;
            2)
                step2_create_hive_tables
                ;;
            3)
                step3_spark_clean
                ;;
            4)
                step4_spark_aggregate
                ;;
            5)
                step5_spark_analyze
                ;;
            6)
                step6_export_mysql
                ;;
            7)
                step7_spark_recommend
                ;;
            8)
                step8_spark_advanced
                ;;
            9)
                step9_spark_prediction
                ;;
            *)
                print_error "æ— æ•ˆçš„æ­¥éª¤å·: $STEP (æœ‰æ•ˆèŒƒå›´: 1-9)"
                show_usage
                exit 1
                ;;
        esac
        
        if [ $? -eq 0 ]; then
            echo ""
            print_success "âœ“ æ­¥éª¤${STEP}æ‰§è¡Œå®Œæˆ"
            echo "éªŒè¯ç»“æœ: bash verify.sh ${STEP}"
        else
            echo ""
            print_error "âœ— æ­¥éª¤${STEP}æ‰§è¡Œå¤±è´¥"
            exit 1
        fi
    fi
}

main "$@"
